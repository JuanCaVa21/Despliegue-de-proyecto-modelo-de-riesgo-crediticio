version: '3.8'

# Creamos los servicios de DOCKER a usar
services:
  
  # Para que en Airflow se guarden los datos es necesario usar una base de datos, en este caso Postgres
  postgres:
    image: postgres:13 # Imagen compatible con Airflow
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always

  # Esto fue el mayor reto, ya que Airflow no esta completamente optimizado para correr en Windows, por lo que tuve que crear una imagen personalizada con Dockerfile para asi poder abrir  la interfaz grafica
  airflow-webserver:
    build: .     # Con esto usamos la imagen que creamos para Airflow
    command: webserver
    ports:
      - "8080:8080" # Puerto del localhost para acceder a la interfaz de Airflow
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 30s
      retries: 5
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
    # Variables de entorno compartidas entre webserver, scheduler e init 
    environment: &airflow-common-env
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'  # <--- Ahorra mucha memoria. Gracias Gemini por no dejar que mi PC explote
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'

    ########## ISSUE #########
    # Esta parte representa las carpetas que se comparten entre el local y el contenedor, es decir como sabe Airflow donde esta cada cosa que le digo que tengo. El principal problemas
    # radica en que no se esta leyendo el archivo de la base de datos Base_de_datos.xlsx. Debe ser o un problema de la configuracion del contenedor o del script cargar_datos.py
    volumes: &airflow-common-volumes
      - ./dags:/opt/airflow/dags # Donde se guarda el script con el DAG
      - ./plugins:/opt/airflow/plugins # Necesarias
      - ./logs:/opt/airflow/logs # Necesarias
      - ./data:/opt/airflow/data  # Los datos a utlizar
      - ./mlops_pipeline:/opt/airflow/mlops_pipeline/scr # Ruta de los scrips de cargar_datos y ft_engineering

  # Esto es la parte central de nuestro contenedor de Airflow, el scheduler es el que se encarga de ejecutar los DAGs en los tiempos programados
  airflow-scheduler:
    build: .                
    command: scheduler
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
    environment: *airflow-common-env
    volumes: *airflow-common-volumes

  # Esto solo se ejecuta una vez que es cuando crea la BD y el usuario Admin para poder entrar a la interfaz grafica de Airflow
  airflow-init:
    build: .
    entrypoint: /bin/bash
    # Este comando crea la DB y un usuario Admin con contraseña admin 
    # Puedes cambiar el email y la contraseña a tu gusto
    command: >
      -c "airflow db init &&
          airflow users create
          --username admin
          --firstname Juan
          --lastname V
          --role Admin
          --email juancvanegas216@gmail.com 
          --password admin"
    environment: *airflow-common-env
    depends_on:
      postgres:
        condition: service_healthy
    volumes: *airflow-common-volumes

# Esto es para crear un volumen que siempre va a estar guardando los datos
volumes:
  postgres-db-volume: